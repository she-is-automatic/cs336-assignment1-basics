{
  "gpt2_small": {
    "vocab_size": 50257,
    "seq_len": 1024,
    "num_layers": 12,
    "num_heads": 12,
    "d_model": 768,
    "d_ff": 3072
  },
  "gpt2_medium": {
    "vocab_size": 50257,
    "seq_len": 1024,
    "num_layers": 24,
    "num_heads": 16,
    "d_model": 1024,
    "d_ff": 4096
  },
  "gpt2_large": {
    "vocab_size": 50257,
    "seq_len": 1024,
    "num_layers": 36,
    "num_heads": 20,
    "d_model": 1280,
    "d_ff": 5120
  },
  "gpt2_xl": {
    "vocab_size": 50257,
    "seq_len": 1024,
    "num_layers": 48,
    "num_heads": 25,
    "d_model": 1600,
    "d_ff": 6400
  },
    "gpt2_xl_long_context": {
    "vocab_size": 50257,
    "seq_len": 16384,
    "num_layers": 48,
    "num_heads": 25,
    "d_model": 1600,
    "d_ff": 6400
  }
}